{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All possible \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from numpy import genfromtxt\n",
    "import csv\n",
    "import numpy as np\n",
    "business = {}\n",
    "allcat = set()\n",
    "with open(\"/Users/yuyanzhang/Desktop/CMU/10605/yelp/10805-YelpChallenge/dataset/business.json\",'r') as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        b_id = line['business_id']\n",
    "        #Get all possible business category\n",
    "        cat = line['categories']\n",
    "        for c in cat:\n",
    "            allcat.add(c)\n",
    "        business[b_id] = line\n",
    "        \n",
    "print(\"All possible \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240\n"
     ]
    }
   ],
   "source": [
    "allcat = (list)(allcat)\n",
    "allcat = {k: v for v, k in enumerate(allcat)}\n",
    "num_cat = len(allcat)\n",
    "print(num_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['---ikxExF5hcu8H6gsUizQ' '10' '0']\n"
     ]
    }
   ],
   "source": [
    "def load_label(path, num_class = 2):\n",
    "    with open(path, 'rb') as f:\n",
    "        reader = csv.reader(f,delimiter='\\t')\n",
    "        labels = np.array((list)(reader))\n",
    "    return labels\n",
    "subset_path = \"../config/labels.csv\"\n",
    "labels = load_label(subset_path)\n",
    "print(labels[0])\n",
    "\n",
    "\n",
    "#Create a hash map for checking if a data exist in the subset\n",
    "sub = {}\n",
    "for y in labels:\n",
    "    sub[y[0]] = [y[1],y[2]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1tbqnDZ5XQO_iE9giRRuIA\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000data point scanned\n",
      "100000data point scanned\n",
      "150000data point scanned\n",
      "10000subset data point processed\n",
      "200000data point scanned\n",
      "250000data point scanned\n",
      "300000data point scanned\n",
      "20000subset data point processed\n",
      "350000data point scanned\n",
      "400000data point scanned\n",
      "450000data point scanned\n",
      "30000subset data point processed\n",
      "500000data point scanned\n",
      "550000data point scanned\n",
      "600000data point scanned\n",
      "40000subset data point processed\n",
      "650000data point scanned\n",
      "700000data point scanned\n",
      "750000data point scanned\n",
      "50000subset data point processed\n",
      "800000data point scanned\n",
      "850000data point scanned\n",
      "900000data point scanned\n",
      "60000subset data point processed\n",
      "950000data point scanned\n",
      "1000000data point scanned\n",
      "1050000data point scanned\n",
      "70000subset data point processed\n",
      "1100000data point scanned\n",
      "1150000data point scanned\n",
      "1200000data point scanned\n",
      "80000subset data point processed\n",
      "1250000data point scanned\n",
      "1300000data point scanned\n",
      "1350000data point scanned\n",
      "1400000data point scanned\n",
      "90000subset data point processed\n",
      "1450000data point scanned\n",
      "1500000data point scanned\n",
      "1550000data point scanned\n",
      "100000subset data point processed\n",
      "1600000data point scanned\n",
      "1650000data point scanned\n",
      "1700000data point scanned\n",
      "110000subset data point processed\n",
      "1750000data point scanned\n",
      "1800000data point scanned\n",
      "1850000data point scanned\n",
      "120000subset data point processed\n",
      "1900000data point scanned\n",
      "1950000data point scanned\n",
      "2000000data point scanned\n",
      "130000subset data point processed\n",
      "2050000data point scanned\n",
      "2100000data point scanned\n",
      "2150000data point scanned\n",
      "140000subset data point processed\n",
      "2200000data point scanned\n",
      "2250000data point scanned\n",
      "2300000data point scanned\n",
      "150000subset data point processed\n",
      "2350000data point scanned\n",
      "2400000data point scanned\n",
      "2450000data point scanned\n",
      "160000subset data point processed\n",
      "2500000data point scanned\n",
      "2550000data point scanned\n",
      "2600000data point scanned\n",
      "2650000data point scanned\n",
      "170000subset data point processed\n",
      "2700000data point scanned\n",
      "2750000data point scanned\n",
      "2800000data point scanned\n",
      "180000subset data point processed\n",
      "2850000data point scanned\n",
      "2900000data point scanned\n",
      "2950000data point scanned\n",
      "190000subset data point processed\n",
      "3000000data point scanned\n",
      "3050000data point scanned\n",
      "3100000data point scanned\n",
      "200000subset data point processed\n",
      "3150000data point scanned\n",
      "3200000data point scanned\n",
      "3250000data point scanned\n",
      "210000subset data point processed\n",
      "3300000data point scanned\n",
      "3350000data point scanned\n",
      "3400000data point scanned\n",
      "220000subset data point processed\n",
      "3450000data point scanned\n",
      "3500000data point scanned\n",
      "3550000data point scanned\n",
      "230000subset data point processed\n",
      "3600000data point scanned\n",
      "3650000data point scanned\n",
      "3700000data point scanned\n",
      "240000subset data point processed\n",
      "3750000data point scanned\n",
      "3800000data point scanned\n",
      "3850000data point scanned\n",
      "250000subset data point processed\n",
      "3900000data point scanned\n",
      "3950000data point scanned\n",
      "4000000data point scanned\n",
      "4050000data point scanned\n",
      "260000subset data point processed\n",
      "4100000data point scanned\n",
      "4150000data point scanned\n",
      "4200000data point scanned\n",
      "270000subset data point processed\n",
      "4250000data point scanned\n",
      "4300000data point scanned\n",
      "4350000data point scanned\n",
      "280000subset data point processed\n",
      "4400000data point scanned\n",
      "4450000data point scanned\n",
      "4500000data point scanned\n",
      "290000subset data point processed\n",
      "4550000data point scanned\n",
      "4600000data point scanned\n",
      "4650000data point scanned\n",
      "300000subset data point processed\n",
      "4700000data point scanned\n",
      "Done writing time stamps to file\n"
     ]
    }
   ],
   "source": [
    "unsorted_meta = {}\n",
    "with open(\"/Users/yuyanzhang/Desktop/CMU/10605/yelp/10805-YelpChallenge/dataset/review.json\") as f:\n",
    "    count = 0\n",
    "    subset_count = 0\n",
    "    with open('../feature/business_meta.csv','w') as f1:\n",
    "        writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n',)\n",
    "        for row in f:\n",
    "            count += 1\n",
    "            if count % 50000 == 0:\n",
    "                print(\"\".join([(str)(count),\"data point scanned\"]))\n",
    "            row = json.loads(row)\n",
    "            k = row['review_id']\n",
    "            #print(sub[k][1])\n",
    "            if not (sub.has_key(k)):\n",
    "                continue\n",
    "            else:\n",
    "                #Current data is in the subset, write to file\n",
    "                #Get business meta associated with review\n",
    "                review_b_id = row['business_id']\n",
    "#                 print(row)\n",
    "#                 print(labels[subset_count])\n",
    "                business_meta = business[review_b_id]\n",
    "\n",
    "                #Reformat the meta data to write to file\n",
    "#                 print(business_meta.keys())\n",
    "                category = business_meta['categories']\n",
    "\n",
    "                cat = [0]*(num_cat)\n",
    "                for c in category:\n",
    "                    idx= allcat[c]\n",
    "                    cat[idx] = 1\n",
    "                towrite = [k,business_meta['longitude'], business_meta['latitude'],\n",
    "                           #business_meta['state'],business_meta['postal_code'],\n",
    "                          business_meta['stars'],business_meta['review_count']]\n",
    "                towrite.extend(cat)\n",
    "                #writer.writerow(towrite)\n",
    "                unsorted_meta[k] = towrite\n",
    "                subset_count += 1\n",
    "                if (subset_count %10000 == 0): #Print process every 10000 data points\n",
    "                    print(\"\".join([(str)(subset_count),\"subset data point processed\"]))\n",
    "    print(\"Done writing time stamps to file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open('../feature/business_meta.csv','w') as f1:\n",
    "    writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n',)\n",
    "    for y in labels:\n",
    "        match_id = y[0]\n",
    "        writer.writerow(unsorted_meta[match_id])\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
